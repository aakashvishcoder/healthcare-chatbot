{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.881844380403458,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01440922190201729,
      "grad_norm": 2.005492925643921,
      "learning_rate": 4.9783861671469746e-05,
      "loss": 2.4377,
      "step": 10
    },
    {
      "epoch": 0.02881844380403458,
      "grad_norm": 2.3535525798797607,
      "learning_rate": 4.954370797310279e-05,
      "loss": 2.2625,
      "step": 20
    },
    {
      "epoch": 0.043227665706051875,
      "grad_norm": 2.408296585083008,
      "learning_rate": 4.9303554274735835e-05,
      "loss": 2.2281,
      "step": 30
    },
    {
      "epoch": 0.05763688760806916,
      "grad_norm": 2.6165621280670166,
      "learning_rate": 4.906340057636888e-05,
      "loss": 2.1721,
      "step": 40
    },
    {
      "epoch": 0.07204610951008646,
      "grad_norm": 2.320629596710205,
      "learning_rate": 4.882324687800192e-05,
      "loss": 2.163,
      "step": 50
    },
    {
      "epoch": 0.08645533141210375,
      "grad_norm": 1.9985711574554443,
      "learning_rate": 4.858309317963497e-05,
      "loss": 2.1381,
      "step": 60
    },
    {
      "epoch": 0.10086455331412104,
      "grad_norm": 2.043256998062134,
      "learning_rate": 4.8342939481268014e-05,
      "loss": 2.1596,
      "step": 70
    },
    {
      "epoch": 0.11527377521613832,
      "grad_norm": 2.275562047958374,
      "learning_rate": 4.810278578290106e-05,
      "loss": 2.0668,
      "step": 80
    },
    {
      "epoch": 0.12968299711815562,
      "grad_norm": 3.4024200439453125,
      "learning_rate": 4.7862632084534104e-05,
      "loss": 2.1104,
      "step": 90
    },
    {
      "epoch": 0.1440922190201729,
      "grad_norm": 2.6831960678100586,
      "learning_rate": 4.762247838616715e-05,
      "loss": 2.1195,
      "step": 100
    },
    {
      "epoch": 0.1585014409221902,
      "grad_norm": 3.097487211227417,
      "learning_rate": 4.7382324687800194e-05,
      "loss": 2.1503,
      "step": 110
    },
    {
      "epoch": 0.1729106628242075,
      "grad_norm": 2.421741247177124,
      "learning_rate": 4.7142170989433235e-05,
      "loss": 2.0776,
      "step": 120
    },
    {
      "epoch": 0.1873198847262248,
      "grad_norm": 3.3689377307891846,
      "learning_rate": 4.690201729106628e-05,
      "loss": 2.0845,
      "step": 130
    },
    {
      "epoch": 0.2017291066282421,
      "grad_norm": 2.7068228721618652,
      "learning_rate": 4.666186359269933e-05,
      "loss": 2.0577,
      "step": 140
    },
    {
      "epoch": 0.21613832853025935,
      "grad_norm": 2.278045654296875,
      "learning_rate": 4.642170989433238e-05,
      "loss": 2.0923,
      "step": 150
    },
    {
      "epoch": 0.23054755043227665,
      "grad_norm": 2.2536823749542236,
      "learning_rate": 4.618155619596542e-05,
      "loss": 2.1476,
      "step": 160
    },
    {
      "epoch": 0.24495677233429394,
      "grad_norm": 2.255333185195923,
      "learning_rate": 4.594140249759846e-05,
      "loss": 2.0227,
      "step": 170
    },
    {
      "epoch": 0.25936599423631124,
      "grad_norm": 2.7524757385253906,
      "learning_rate": 4.570124879923151e-05,
      "loss": 2.1006,
      "step": 180
    },
    {
      "epoch": 0.2737752161383285,
      "grad_norm": 2.7103137969970703,
      "learning_rate": 4.546109510086455e-05,
      "loss": 2.0749,
      "step": 190
    },
    {
      "epoch": 0.2881844380403458,
      "grad_norm": 2.381814479827881,
      "learning_rate": 4.52209414024976e-05,
      "loss": 2.0764,
      "step": 200
    },
    {
      "epoch": 0.3025936599423631,
      "grad_norm": 2.3570003509521484,
      "learning_rate": 4.498078770413065e-05,
      "loss": 2.1129,
      "step": 210
    },
    {
      "epoch": 0.3170028818443804,
      "grad_norm": 2.4231066703796387,
      "learning_rate": 4.4740634005763696e-05,
      "loss": 2.0608,
      "step": 220
    },
    {
      "epoch": 0.3314121037463977,
      "grad_norm": 2.6804850101470947,
      "learning_rate": 4.450048030739674e-05,
      "loss": 2.0667,
      "step": 230
    },
    {
      "epoch": 0.345821325648415,
      "grad_norm": 2.4423844814300537,
      "learning_rate": 4.426032660902978e-05,
      "loss": 2.0227,
      "step": 240
    },
    {
      "epoch": 0.36023054755043227,
      "grad_norm": 2.307080030441284,
      "learning_rate": 4.402017291066283e-05,
      "loss": 2.0211,
      "step": 250
    },
    {
      "epoch": 0.3746397694524496,
      "grad_norm": 2.2691545486450195,
      "learning_rate": 4.378001921229587e-05,
      "loss": 2.0689,
      "step": 260
    },
    {
      "epoch": 0.38904899135446686,
      "grad_norm": 2.5472261905670166,
      "learning_rate": 4.353986551392892e-05,
      "loss": 2.1182,
      "step": 270
    },
    {
      "epoch": 0.4034582132564842,
      "grad_norm": 3.208860397338867,
      "learning_rate": 4.329971181556196e-05,
      "loss": 2.0152,
      "step": 280
    },
    {
      "epoch": 0.41786743515850144,
      "grad_norm": 2.502565860748291,
      "learning_rate": 4.3059558117195006e-05,
      "loss": 2.0701,
      "step": 290
    },
    {
      "epoch": 0.4322766570605187,
      "grad_norm": 2.701859712600708,
      "learning_rate": 4.2819404418828054e-05,
      "loss": 1.9808,
      "step": 300
    },
    {
      "epoch": 0.44668587896253603,
      "grad_norm": 2.494534730911255,
      "learning_rate": 4.2579250720461096e-05,
      "loss": 2.0517,
      "step": 310
    },
    {
      "epoch": 0.4610951008645533,
      "grad_norm": 2.6749024391174316,
      "learning_rate": 4.2339097022094144e-05,
      "loss": 2.13,
      "step": 320
    },
    {
      "epoch": 0.4755043227665706,
      "grad_norm": 2.4016547203063965,
      "learning_rate": 4.2098943323727185e-05,
      "loss": 2.0907,
      "step": 330
    },
    {
      "epoch": 0.4899135446685879,
      "grad_norm": 2.7855448722839355,
      "learning_rate": 4.1858789625360234e-05,
      "loss": 1.9672,
      "step": 340
    },
    {
      "epoch": 0.5043227665706052,
      "grad_norm": 2.960157871246338,
      "learning_rate": 4.1618635926993275e-05,
      "loss": 2.0492,
      "step": 350
    },
    {
      "epoch": 0.5187319884726225,
      "grad_norm": 3.6297390460968018,
      "learning_rate": 4.137848222862632e-05,
      "loss": 1.9681,
      "step": 360
    },
    {
      "epoch": 0.5331412103746398,
      "grad_norm": 2.4486305713653564,
      "learning_rate": 4.113832853025937e-05,
      "loss": 2.0198,
      "step": 370
    },
    {
      "epoch": 0.547550432276657,
      "grad_norm": 3.5275611877441406,
      "learning_rate": 4.089817483189241e-05,
      "loss": 2.0046,
      "step": 380
    },
    {
      "epoch": 0.5619596541786743,
      "grad_norm": 3.4444751739501953,
      "learning_rate": 4.065802113352546e-05,
      "loss": 1.9746,
      "step": 390
    },
    {
      "epoch": 0.5763688760806917,
      "grad_norm": 2.8886005878448486,
      "learning_rate": 4.04178674351585e-05,
      "loss": 1.9417,
      "step": 400
    },
    {
      "epoch": 0.590778097982709,
      "grad_norm": 3.5821595191955566,
      "learning_rate": 4.017771373679155e-05,
      "loss": 2.0077,
      "step": 410
    },
    {
      "epoch": 0.6051873198847262,
      "grad_norm": 3.067779302597046,
      "learning_rate": 3.993756003842459e-05,
      "loss": 1.9211,
      "step": 420
    },
    {
      "epoch": 0.6195965417867435,
      "grad_norm": 3.8343961238861084,
      "learning_rate": 3.969740634005764e-05,
      "loss": 2.0668,
      "step": 430
    },
    {
      "epoch": 0.6340057636887608,
      "grad_norm": 2.7700321674346924,
      "learning_rate": 3.945725264169068e-05,
      "loss": 1.9366,
      "step": 440
    },
    {
      "epoch": 0.6484149855907781,
      "grad_norm": 2.7619242668151855,
      "learning_rate": 3.921709894332373e-05,
      "loss": 2.0885,
      "step": 450
    },
    {
      "epoch": 0.6628242074927954,
      "grad_norm": 2.808223247528076,
      "learning_rate": 3.897694524495678e-05,
      "loss": 1.975,
      "step": 460
    },
    {
      "epoch": 0.6772334293948127,
      "grad_norm": 3.1817355155944824,
      "learning_rate": 3.873679154658982e-05,
      "loss": 1.937,
      "step": 470
    },
    {
      "epoch": 0.69164265129683,
      "grad_norm": 2.6110527515411377,
      "learning_rate": 3.849663784822287e-05,
      "loss": 1.9829,
      "step": 480
    },
    {
      "epoch": 0.7060518731988472,
      "grad_norm": 2.72148060798645,
      "learning_rate": 3.825648414985591e-05,
      "loss": 2.0342,
      "step": 490
    },
    {
      "epoch": 0.7204610951008645,
      "grad_norm": 2.9785678386688232,
      "learning_rate": 3.801633045148896e-05,
      "loss": 2.0078,
      "step": 500
    },
    {
      "epoch": 0.7348703170028819,
      "grad_norm": 2.903787136077881,
      "learning_rate": 3.7776176753122e-05,
      "loss": 1.9614,
      "step": 510
    },
    {
      "epoch": 0.7492795389048992,
      "grad_norm": 3.188555955886841,
      "learning_rate": 3.7536023054755046e-05,
      "loss": 2.127,
      "step": 520
    },
    {
      "epoch": 0.7636887608069164,
      "grad_norm": 3.3019402027130127,
      "learning_rate": 3.7295869356388094e-05,
      "loss": 2.054,
      "step": 530
    },
    {
      "epoch": 0.7780979827089337,
      "grad_norm": 3.243587017059326,
      "learning_rate": 3.7055715658021136e-05,
      "loss": 2.0089,
      "step": 540
    },
    {
      "epoch": 0.792507204610951,
      "grad_norm": 3.4511632919311523,
      "learning_rate": 3.6815561959654184e-05,
      "loss": 1.9458,
      "step": 550
    },
    {
      "epoch": 0.8069164265129684,
      "grad_norm": 3.332986831665039,
      "learning_rate": 3.6575408261287225e-05,
      "loss": 1.9959,
      "step": 560
    },
    {
      "epoch": 0.8213256484149856,
      "grad_norm": 2.8409037590026855,
      "learning_rate": 3.6335254562920274e-05,
      "loss": 1.9953,
      "step": 570
    },
    {
      "epoch": 0.8357348703170029,
      "grad_norm": 3.1384546756744385,
      "learning_rate": 3.6095100864553315e-05,
      "loss": 1.97,
      "step": 580
    },
    {
      "epoch": 0.8501440922190202,
      "grad_norm": 3.2169246673583984,
      "learning_rate": 3.5854947166186356e-05,
      "loss": 1.9746,
      "step": 590
    },
    {
      "epoch": 0.8645533141210374,
      "grad_norm": 3.1287691593170166,
      "learning_rate": 3.5614793467819404e-05,
      "loss": 1.9891,
      "step": 600
    },
    {
      "epoch": 0.8789625360230547,
      "grad_norm": 3.7772674560546875,
      "learning_rate": 3.539865513928914e-05,
      "loss": 2.034,
      "step": 610
    },
    {
      "epoch": 0.8933717579250721,
      "grad_norm": 4.891062259674072,
      "learning_rate": 3.515850144092219e-05,
      "loss": 1.9647,
      "step": 620
    },
    {
      "epoch": 0.9077809798270894,
      "grad_norm": 3.916736602783203,
      "learning_rate": 3.491834774255524e-05,
      "loss": 2.0352,
      "step": 630
    },
    {
      "epoch": 0.9221902017291066,
      "grad_norm": 4.016970634460449,
      "learning_rate": 3.4678194044188286e-05,
      "loss": 1.9351,
      "step": 640
    },
    {
      "epoch": 0.9365994236311239,
      "grad_norm": 6.494422435760498,
      "learning_rate": 3.443804034582133e-05,
      "loss": 2.0816,
      "step": 650
    },
    {
      "epoch": 0.9510086455331412,
      "grad_norm": 3.3469624519348145,
      "learning_rate": 3.4197886647454375e-05,
      "loss": 1.9882,
      "step": 660
    },
    {
      "epoch": 0.9654178674351584,
      "grad_norm": 2.5070042610168457,
      "learning_rate": 3.3957732949087416e-05,
      "loss": 1.9021,
      "step": 670
    },
    {
      "epoch": 0.9798270893371758,
      "grad_norm": 2.9101674556732178,
      "learning_rate": 3.371757925072046e-05,
      "loss": 1.9309,
      "step": 680
    },
    {
      "epoch": 0.9942363112391931,
      "grad_norm": 5.448349475860596,
      "learning_rate": 3.3477425552353506e-05,
      "loss": 1.9349,
      "step": 690
    },
    {
      "epoch": 1.0086455331412103,
      "grad_norm": 4.34535026550293,
      "learning_rate": 3.3237271853986554e-05,
      "loss": 1.976,
      "step": 700
    },
    {
      "epoch": 1.0230547550432276,
      "grad_norm": 4.123495578765869,
      "learning_rate": 3.29971181556196e-05,
      "loss": 1.9461,
      "step": 710
    },
    {
      "epoch": 1.037463976945245,
      "grad_norm": 5.723149299621582,
      "learning_rate": 3.2756964457252644e-05,
      "loss": 1.8359,
      "step": 720
    },
    {
      "epoch": 1.0518731988472623,
      "grad_norm": 4.121103763580322,
      "learning_rate": 3.251681075888569e-05,
      "loss": 1.9906,
      "step": 730
    },
    {
      "epoch": 1.0662824207492796,
      "grad_norm": 3.7073378562927246,
      "learning_rate": 3.227665706051873e-05,
      "loss": 2.0092,
      "step": 740
    },
    {
      "epoch": 1.080691642651297,
      "grad_norm": 4.644664764404297,
      "learning_rate": 3.2036503362151775e-05,
      "loss": 1.8524,
      "step": 750
    },
    {
      "epoch": 1.0951008645533142,
      "grad_norm": 3.32611346244812,
      "learning_rate": 3.179634966378482e-05,
      "loss": 1.9483,
      "step": 760
    },
    {
      "epoch": 1.1095100864553313,
      "grad_norm": 2.9163506031036377,
      "learning_rate": 3.1556195965417864e-05,
      "loss": 1.9104,
      "step": 770
    },
    {
      "epoch": 1.1239193083573487,
      "grad_norm": 3.712507724761963,
      "learning_rate": 3.131604226705091e-05,
      "loss": 1.9683,
      "step": 780
    },
    {
      "epoch": 1.138328530259366,
      "grad_norm": 3.5735344886779785,
      "learning_rate": 3.107588856868396e-05,
      "loss": 1.8026,
      "step": 790
    },
    {
      "epoch": 1.1527377521613833,
      "grad_norm": 3.283520460128784,
      "learning_rate": 3.083573487031701e-05,
      "loss": 1.9831,
      "step": 800
    },
    {
      "epoch": 1.1671469740634006,
      "grad_norm": 4.205513000488281,
      "learning_rate": 3.059558117195005e-05,
      "loss": 1.9085,
      "step": 810
    },
    {
      "epoch": 1.181556195965418,
      "grad_norm": 3.0381810665130615,
      "learning_rate": 3.035542747358309e-05,
      "loss": 2.0458,
      "step": 820
    },
    {
      "epoch": 1.195965417867435,
      "grad_norm": 3.1768507957458496,
      "learning_rate": 3.011527377521614e-05,
      "loss": 1.9421,
      "step": 830
    },
    {
      "epoch": 1.2103746397694524,
      "grad_norm": 3.8116509914398193,
      "learning_rate": 2.9875120076849184e-05,
      "loss": 1.9284,
      "step": 840
    },
    {
      "epoch": 1.2247838616714697,
      "grad_norm": 4.5096330642700195,
      "learning_rate": 2.9634966378482233e-05,
      "loss": 2.0106,
      "step": 850
    },
    {
      "epoch": 1.239193083573487,
      "grad_norm": 4.166524410247803,
      "learning_rate": 2.9394812680115274e-05,
      "loss": 1.923,
      "step": 860
    },
    {
      "epoch": 1.2536023054755043,
      "grad_norm": 3.2963271141052246,
      "learning_rate": 2.9154658981748322e-05,
      "loss": 1.8339,
      "step": 870
    },
    {
      "epoch": 1.2680115273775217,
      "grad_norm": 3.9956841468811035,
      "learning_rate": 2.8914505283381367e-05,
      "loss": 1.7549,
      "step": 880
    },
    {
      "epoch": 1.282420749279539,
      "grad_norm": 3.9568240642547607,
      "learning_rate": 2.867435158501441e-05,
      "loss": 1.9246,
      "step": 890
    },
    {
      "epoch": 1.2968299711815563,
      "grad_norm": 3.7564468383789062,
      "learning_rate": 2.845821325648415e-05,
      "loss": 1.902,
      "step": 900
    },
    {
      "epoch": 1.3112391930835736,
      "grad_norm": 4.152988910675049,
      "learning_rate": 2.824207492795389e-05,
      "loss": 1.8922,
      "step": 910
    },
    {
      "epoch": 1.3256484149855907,
      "grad_norm": 4.254019737243652,
      "learning_rate": 2.8001921229586936e-05,
      "loss": 1.8756,
      "step": 920
    },
    {
      "epoch": 1.340057636887608,
      "grad_norm": 3.612846612930298,
      "learning_rate": 2.7761767531219985e-05,
      "loss": 1.916,
      "step": 930
    },
    {
      "epoch": 1.3544668587896254,
      "grad_norm": 3.922696113586426,
      "learning_rate": 2.7521613832853026e-05,
      "loss": 1.8856,
      "step": 940
    },
    {
      "epoch": 1.3688760806916427,
      "grad_norm": 5.2322797775268555,
      "learning_rate": 2.728146013448607e-05,
      "loss": 1.9048,
      "step": 950
    },
    {
      "epoch": 1.38328530259366,
      "grad_norm": 3.09028959274292,
      "learning_rate": 2.704130643611912e-05,
      "loss": 1.8958,
      "step": 960
    },
    {
      "epoch": 1.397694524495677,
      "grad_norm": 3.429460287094116,
      "learning_rate": 2.680115273775216e-05,
      "loss": 1.8951,
      "step": 970
    },
    {
      "epoch": 1.4121037463976944,
      "grad_norm": 4.087752342224121,
      "learning_rate": 2.656099903938521e-05,
      "loss": 1.8815,
      "step": 980
    },
    {
      "epoch": 1.4265129682997117,
      "grad_norm": 3.5688929557800293,
      "learning_rate": 2.6320845341018253e-05,
      "loss": 1.904,
      "step": 990
    },
    {
      "epoch": 1.440922190201729,
      "grad_norm": 9.386311531066895,
      "learning_rate": 2.60806916426513e-05,
      "loss": 1.9275,
      "step": 1000
    },
    {
      "epoch": 1.4553314121037464,
      "grad_norm": 3.4017112255096436,
      "learning_rate": 2.5840537944284343e-05,
      "loss": 1.8843,
      "step": 1010
    },
    {
      "epoch": 1.4697406340057637,
      "grad_norm": 3.8741116523742676,
      "learning_rate": 2.5600384245917388e-05,
      "loss": 1.8262,
      "step": 1020
    },
    {
      "epoch": 1.484149855907781,
      "grad_norm": 3.6888132095336914,
      "learning_rate": 2.5360230547550436e-05,
      "loss": 1.9469,
      "step": 1030
    },
    {
      "epoch": 1.4985590778097984,
      "grad_norm": 3.6385350227355957,
      "learning_rate": 2.5120076849183477e-05,
      "loss": 1.8643,
      "step": 1040
    },
    {
      "epoch": 1.5129682997118157,
      "grad_norm": 6.256011009216309,
      "learning_rate": 2.4879923150816522e-05,
      "loss": 1.8125,
      "step": 1050
    },
    {
      "epoch": 1.527377521613833,
      "grad_norm": 3.9777371883392334,
      "learning_rate": 2.4639769452449567e-05,
      "loss": 1.9305,
      "step": 1060
    },
    {
      "epoch": 1.54178674351585,
      "grad_norm": 4.55777645111084,
      "learning_rate": 2.4399615754082615e-05,
      "loss": 1.8494,
      "step": 1070
    },
    {
      "epoch": 1.5561959654178674,
      "grad_norm": 3.762483596801758,
      "learning_rate": 2.415946205571566e-05,
      "loss": 1.9894,
      "step": 1080
    },
    {
      "epoch": 1.5706051873198847,
      "grad_norm": 3.907400131225586,
      "learning_rate": 2.3919308357348704e-05,
      "loss": 1.8338,
      "step": 1090
    },
    {
      "epoch": 1.585014409221902,
      "grad_norm": 4.219057559967041,
      "learning_rate": 2.367915465898175e-05,
      "loss": 1.9651,
      "step": 1100
    },
    {
      "epoch": 1.5994236311239192,
      "grad_norm": 4.9710164070129395,
      "learning_rate": 2.3439000960614797e-05,
      "loss": 1.7862,
      "step": 1110
    },
    {
      "epoch": 1.6138328530259365,
      "grad_norm": 3.7673392295837402,
      "learning_rate": 2.319884726224784e-05,
      "loss": 1.8342,
      "step": 1120
    },
    {
      "epoch": 1.6282420749279538,
      "grad_norm": 3.905362606048584,
      "learning_rate": 2.2958693563880883e-05,
      "loss": 1.856,
      "step": 1130
    },
    {
      "epoch": 1.6426512968299711,
      "grad_norm": 4.401906490325928,
      "learning_rate": 2.2718539865513928e-05,
      "loss": 1.9176,
      "step": 1140
    },
    {
      "epoch": 1.6570605187319885,
      "grad_norm": 4.992089748382568,
      "learning_rate": 2.2478386167146976e-05,
      "loss": 1.9489,
      "step": 1150
    },
    {
      "epoch": 1.6714697406340058,
      "grad_norm": 5.015787124633789,
      "learning_rate": 2.223823246878002e-05,
      "loss": 1.8343,
      "step": 1160
    },
    {
      "epoch": 1.685878962536023,
      "grad_norm": 3.7594642639160156,
      "learning_rate": 2.1998078770413066e-05,
      "loss": 1.9442,
      "step": 1170
    },
    {
      "epoch": 1.7002881844380404,
      "grad_norm": 6.632864475250244,
      "learning_rate": 2.175792507204611e-05,
      "loss": 1.7714,
      "step": 1180
    },
    {
      "epoch": 1.7146974063400577,
      "grad_norm": 3.945474147796631,
      "learning_rate": 2.1517771373679156e-05,
      "loss": 1.9195,
      "step": 1190
    },
    {
      "epoch": 1.729106628242075,
      "grad_norm": 4.371417999267578,
      "learning_rate": 2.12776176753122e-05,
      "loss": 1.8879,
      "step": 1200
    },
    {
      "epoch": 1.7435158501440924,
      "grad_norm": 4.271308422088623,
      "learning_rate": 2.1037463976945245e-05,
      "loss": 1.9095,
      "step": 1210
    },
    {
      "epoch": 1.7579250720461095,
      "grad_norm": 4.578153610229492,
      "learning_rate": 2.079731027857829e-05,
      "loss": 1.8747,
      "step": 1220
    },
    {
      "epoch": 1.7723342939481268,
      "grad_norm": 5.127931118011475,
      "learning_rate": 2.0557156580211338e-05,
      "loss": 1.8803,
      "step": 1230
    },
    {
      "epoch": 1.7867435158501441,
      "grad_norm": 7.6385498046875,
      "learning_rate": 2.0317002881844383e-05,
      "loss": 1.7625,
      "step": 1240
    },
    {
      "epoch": 1.8011527377521612,
      "grad_norm": 5.119089603424072,
      "learning_rate": 2.0076849183477424e-05,
      "loss": 1.8237,
      "step": 1250
    },
    {
      "epoch": 1.8155619596541785,
      "grad_norm": 4.429110527038574,
      "learning_rate": 1.9836695485110472e-05,
      "loss": 1.9298,
      "step": 1260
    },
    {
      "epoch": 1.8299711815561959,
      "grad_norm": 3.3381192684173584,
      "learning_rate": 1.9596541786743517e-05,
      "loss": 1.9225,
      "step": 1270
    },
    {
      "epoch": 1.8443804034582132,
      "grad_norm": 3.8278214931488037,
      "learning_rate": 1.9356388088376562e-05,
      "loss": 1.8003,
      "step": 1280
    },
    {
      "epoch": 1.8587896253602305,
      "grad_norm": 4.53961181640625,
      "learning_rate": 1.9116234390009607e-05,
      "loss": 1.8733,
      "step": 1290
    },
    {
      "epoch": 1.8731988472622478,
      "grad_norm": 3.947943925857544,
      "learning_rate": 1.887608069164265e-05,
      "loss": 1.9369,
      "step": 1300
    },
    {
      "epoch": 1.8876080691642652,
      "grad_norm": 4.273275375366211,
      "learning_rate": 1.86359269932757e-05,
      "loss": 1.831,
      "step": 1310
    },
    {
      "epoch": 1.9020172910662825,
      "grad_norm": 3.706117630004883,
      "learning_rate": 1.839577329490874e-05,
      "loss": 1.9437,
      "step": 1320
    },
    {
      "epoch": 1.9164265129682998,
      "grad_norm": 4.145586967468262,
      "learning_rate": 1.8155619596541786e-05,
      "loss": 1.952,
      "step": 1330
    },
    {
      "epoch": 1.9308357348703171,
      "grad_norm": 3.2652246952056885,
      "learning_rate": 1.7915465898174834e-05,
      "loss": 1.9449,
      "step": 1340
    },
    {
      "epoch": 1.9452449567723344,
      "grad_norm": 4.637274265289307,
      "learning_rate": 1.767531219980788e-05,
      "loss": 1.9144,
      "step": 1350
    },
    {
      "epoch": 1.9596541786743515,
      "grad_norm": 4.7933244705200195,
      "learning_rate": 1.7435158501440924e-05,
      "loss": 1.877,
      "step": 1360
    },
    {
      "epoch": 1.9740634005763689,
      "grad_norm": 5.8249406814575195,
      "learning_rate": 1.7195004803073968e-05,
      "loss": 1.7148,
      "step": 1370
    },
    {
      "epoch": 1.9884726224783862,
      "grad_norm": 4.74698543548584,
      "learning_rate": 1.6954851104707013e-05,
      "loss": 1.9387,
      "step": 1380
    },
    {
      "epoch": 2.0028818443804033,
      "grad_norm": 3.534080743789673,
      "learning_rate": 1.6714697406340058e-05,
      "loss": 1.8586,
      "step": 1390
    },
    {
      "epoch": 2.0172910662824206,
      "grad_norm": 4.191646099090576,
      "learning_rate": 1.6474543707973103e-05,
      "loss": 1.9166,
      "step": 1400
    },
    {
      "epoch": 2.031700288184438,
      "grad_norm": 4.116510391235352,
      "learning_rate": 1.6234390009606147e-05,
      "loss": 1.8658,
      "step": 1410
    },
    {
      "epoch": 2.0461095100864553,
      "grad_norm": 4.472327709197998,
      "learning_rate": 1.5994236311239196e-05,
      "loss": 1.821,
      "step": 1420
    },
    {
      "epoch": 2.0605187319884726,
      "grad_norm": 5.936474323272705,
      "learning_rate": 1.575408261287224e-05,
      "loss": 1.8159,
      "step": 1430
    },
    {
      "epoch": 2.07492795389049,
      "grad_norm": 6.903547763824463,
      "learning_rate": 1.5513928914505285e-05,
      "loss": 1.6268,
      "step": 1440
    },
    {
      "epoch": 2.089337175792507,
      "grad_norm": 6.520665168762207,
      "learning_rate": 1.5273775216138326e-05,
      "loss": 1.8435,
      "step": 1450
    },
    {
      "epoch": 2.1037463976945245,
      "grad_norm": 4.213863849639893,
      "learning_rate": 1.5033621517771373e-05,
      "loss": 1.8026,
      "step": 1460
    },
    {
      "epoch": 2.118155619596542,
      "grad_norm": 3.8083386421203613,
      "learning_rate": 1.479346781940442e-05,
      "loss": 1.8062,
      "step": 1470
    },
    {
      "epoch": 2.132564841498559,
      "grad_norm": 5.711339473724365,
      "learning_rate": 1.4553314121037464e-05,
      "loss": 1.8312,
      "step": 1480
    },
    {
      "epoch": 2.1469740634005765,
      "grad_norm": 4.324911594390869,
      "learning_rate": 1.431316042267051e-05,
      "loss": 1.8138,
      "step": 1490
    },
    {
      "epoch": 2.161383285302594,
      "grad_norm": 5.854725360870361,
      "learning_rate": 1.4073006724303555e-05,
      "loss": 1.8217,
      "step": 1500
    },
    {
      "epoch": 2.175792507204611,
      "grad_norm": 4.482296943664551,
      "learning_rate": 1.3832853025936602e-05,
      "loss": 1.784,
      "step": 1510
    },
    {
      "epoch": 2.1902017291066285,
      "grad_norm": 6.359112739562988,
      "learning_rate": 1.3592699327569643e-05,
      "loss": 1.7846,
      "step": 1520
    },
    {
      "epoch": 2.2046109510086453,
      "grad_norm": 5.87421178817749,
      "learning_rate": 1.335254562920269e-05,
      "loss": 1.8269,
      "step": 1530
    },
    {
      "epoch": 2.2190201729106627,
      "grad_norm": 5.649975299835205,
      "learning_rate": 1.3112391930835735e-05,
      "loss": 1.8506,
      "step": 1540
    },
    {
      "epoch": 2.23342939481268,
      "grad_norm": 4.449149131774902,
      "learning_rate": 1.2872238232468781e-05,
      "loss": 1.8358,
      "step": 1550
    },
    {
      "epoch": 2.2478386167146973,
      "grad_norm": 4.302743911743164,
      "learning_rate": 1.2632084534101826e-05,
      "loss": 1.8624,
      "step": 1560
    },
    {
      "epoch": 2.2622478386167146,
      "grad_norm": 4.940973281860352,
      "learning_rate": 1.239193083573487e-05,
      "loss": 1.7025,
      "step": 1570
    },
    {
      "epoch": 2.276657060518732,
      "grad_norm": 4.737765789031982,
      "learning_rate": 1.2151777137367915e-05,
      "loss": 1.8508,
      "step": 1580
    },
    {
      "epoch": 2.2910662824207493,
      "grad_norm": 4.741371154785156,
      "learning_rate": 1.1911623439000962e-05,
      "loss": 1.8585,
      "step": 1590
    },
    {
      "epoch": 2.3054755043227666,
      "grad_norm": 5.676435947418213,
      "learning_rate": 1.1671469740634005e-05,
      "loss": 1.8479,
      "step": 1600
    },
    {
      "epoch": 2.319884726224784,
      "grad_norm": 6.2906413078308105,
      "learning_rate": 1.1431316042267051e-05,
      "loss": 1.9263,
      "step": 1610
    },
    {
      "epoch": 2.3342939481268012,
      "grad_norm": 5.314864635467529,
      "learning_rate": 1.1191162343900096e-05,
      "loss": 1.7369,
      "step": 1620
    },
    {
      "epoch": 2.3487031700288186,
      "grad_norm": 6.9644775390625,
      "learning_rate": 1.0951008645533143e-05,
      "loss": 1.9164,
      "step": 1630
    },
    {
      "epoch": 2.363112391930836,
      "grad_norm": 5.11686897277832,
      "learning_rate": 1.0710854947166186e-05,
      "loss": 1.754,
      "step": 1640
    },
    {
      "epoch": 2.377521613832853,
      "grad_norm": 5.312232971191406,
      "learning_rate": 1.0470701248799232e-05,
      "loss": 1.8312,
      "step": 1650
    },
    {
      "epoch": 2.39193083573487,
      "grad_norm": 5.35777473449707,
      "learning_rate": 1.0230547550432277e-05,
      "loss": 1.7515,
      "step": 1660
    },
    {
      "epoch": 2.4063400576368874,
      "grad_norm": 5.862275123596191,
      "learning_rate": 9.990393852065322e-06,
      "loss": 1.7534,
      "step": 1670
    },
    {
      "epoch": 2.4207492795389047,
      "grad_norm": 2.6969640254974365,
      "learning_rate": 9.750240153698367e-06,
      "loss": 1.7077,
      "step": 1680
    },
    {
      "epoch": 2.435158501440922,
      "grad_norm": 6.77467155456543,
      "learning_rate": 9.510086455331413e-06,
      "loss": 1.8367,
      "step": 1690
    },
    {
      "epoch": 2.4495677233429394,
      "grad_norm": 5.062165260314941,
      "learning_rate": 9.269932756964458e-06,
      "loss": 1.8054,
      "step": 1700
    },
    {
      "epoch": 2.4639769452449567,
      "grad_norm": 4.053202152252197,
      "learning_rate": 9.029779058597503e-06,
      "loss": 1.8162,
      "step": 1710
    },
    {
      "epoch": 2.478386167146974,
      "grad_norm": 4.972075462341309,
      "learning_rate": 8.789625360230547e-06,
      "loss": 1.8071,
      "step": 1720
    },
    {
      "epoch": 2.4927953890489913,
      "grad_norm": 4.175838947296143,
      "learning_rate": 8.549471661863594e-06,
      "loss": 1.8644,
      "step": 1730
    },
    {
      "epoch": 2.5072046109510087,
      "grad_norm": 5.764858245849609,
      "learning_rate": 8.309317963496639e-06,
      "loss": 1.7642,
      "step": 1740
    },
    {
      "epoch": 2.521613832853026,
      "grad_norm": 5.632154941558838,
      "learning_rate": 8.069164265129683e-06,
      "loss": 1.7777,
      "step": 1750
    },
    {
      "epoch": 2.5360230547550433,
      "grad_norm": 6.620039939880371,
      "learning_rate": 7.829010566762728e-06,
      "loss": 1.7832,
      "step": 1760
    },
    {
      "epoch": 2.5504322766570606,
      "grad_norm": 5.651422500610352,
      "learning_rate": 7.588856868395773e-06,
      "loss": 1.7637,
      "step": 1770
    },
    {
      "epoch": 2.564841498559078,
      "grad_norm": 5.928431987762451,
      "learning_rate": 7.3487031700288185e-06,
      "loss": 1.9667,
      "step": 1780
    },
    {
      "epoch": 2.5792507204610953,
      "grad_norm": 4.206483364105225,
      "learning_rate": 7.108549471661864e-06,
      "loss": 1.8366,
      "step": 1790
    },
    {
      "epoch": 2.5936599423631126,
      "grad_norm": 5.918964385986328,
      "learning_rate": 6.86839577329491e-06,
      "loss": 1.7725,
      "step": 1800
    },
    {
      "epoch": 2.60806916426513,
      "grad_norm": 4.792453765869141,
      "learning_rate": 6.628242074927954e-06,
      "loss": 1.8207,
      "step": 1810
    },
    {
      "epoch": 2.6224783861671472,
      "grad_norm": 4.124084949493408,
      "learning_rate": 6.388088376560999e-06,
      "loss": 1.8258,
      "step": 1820
    },
    {
      "epoch": 2.636887608069164,
      "grad_norm": 5.484281063079834,
      "learning_rate": 6.147934678194044e-06,
      "loss": 1.8395,
      "step": 1830
    },
    {
      "epoch": 2.6512968299711814,
      "grad_norm": 4.977386474609375,
      "learning_rate": 5.90778097982709e-06,
      "loss": 1.8361,
      "step": 1840
    },
    {
      "epoch": 2.6657060518731988,
      "grad_norm": 3.6429812908172607,
      "learning_rate": 5.6676272814601345e-06,
      "loss": 1.9016,
      "step": 1850
    },
    {
      "epoch": 2.680115273775216,
      "grad_norm": 7.15956974029541,
      "learning_rate": 5.427473583093179e-06,
      "loss": 1.7701,
      "step": 1860
    },
    {
      "epoch": 2.6945244956772334,
      "grad_norm": 4.7944016456604,
      "learning_rate": 5.187319884726225e-06,
      "loss": 1.8433,
      "step": 1870
    },
    {
      "epoch": 2.7089337175792507,
      "grad_norm": 4.7256083488464355,
      "learning_rate": 4.94716618635927e-06,
      "loss": 1.7387,
      "step": 1880
    },
    {
      "epoch": 2.723342939481268,
      "grad_norm": 4.817959785461426,
      "learning_rate": 4.707012487992315e-06,
      "loss": 1.8171,
      "step": 1890
    },
    {
      "epoch": 2.7377521613832854,
      "grad_norm": 4.410874843597412,
      "learning_rate": 4.46685878962536e-06,
      "loss": 1.7874,
      "step": 1900
    },
    {
      "epoch": 2.7521613832853027,
      "grad_norm": 5.455307483673096,
      "learning_rate": 4.226705091258405e-06,
      "loss": 1.7531,
      "step": 1910
    },
    {
      "epoch": 2.76657060518732,
      "grad_norm": 5.522407531738281,
      "learning_rate": 3.9865513928914504e-06,
      "loss": 1.8784,
      "step": 1920
    },
    {
      "epoch": 2.7809798270893373,
      "grad_norm": 4.530393123626709,
      "learning_rate": 3.7463976945244956e-06,
      "loss": 1.8424,
      "step": 1930
    },
    {
      "epoch": 2.795389048991354,
      "grad_norm": 5.014055252075195,
      "learning_rate": 3.5062439961575413e-06,
      "loss": 1.9402,
      "step": 1940
    },
    {
      "epoch": 2.8097982708933715,
      "grad_norm": 4.217051982879639,
      "learning_rate": 3.266090297790586e-06,
      "loss": 1.85,
      "step": 1950
    },
    {
      "epoch": 2.824207492795389,
      "grad_norm": 4.832065582275391,
      "learning_rate": 3.0259365994236312e-06,
      "loss": 1.8892,
      "step": 1960
    },
    {
      "epoch": 2.838616714697406,
      "grad_norm": 4.496609210968018,
      "learning_rate": 2.7857829010566764e-06,
      "loss": 1.8003,
      "step": 1970
    },
    {
      "epoch": 2.8530259365994235,
      "grad_norm": 4.495128154754639,
      "learning_rate": 2.5456292026897216e-06,
      "loss": 1.8854,
      "step": 1980
    },
    {
      "epoch": 2.867435158501441,
      "grad_norm": 4.689472675323486,
      "learning_rate": 2.3054755043227664e-06,
      "loss": 1.8019,
      "step": 1990
    },
    {
      "epoch": 2.881844380403458,
      "grad_norm": 3.7265844345092773,
      "learning_rate": 2.0653218059558116e-06,
      "loss": 1.7498,
      "step": 2000
    }
  ],
  "logging_steps": 10,
  "max_steps": 2082,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.7479182682423296e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
